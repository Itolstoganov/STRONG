from Bio.SeqIO.FastaIO import SimpleFastaParser as SFP
import glob
#TODO change paths?
#TODO check that works correctly with subset of samples?
#TODO switch to the gfa input?
rule unitig_profiles:
    input:
        graph="{path}.gfa",
        reads_desc="samples.yaml"
    output:
        "{path}.mult_prof"
    params:
        gp="{path}"
    log:
        "{path}.mult_prof.log"
    threads:
        THREADS
    shell:
        "{SOFT}/unitig-coverage {input.reads_desc} {input.graph} {output} "
        "-k {ASSEMBLY_K} -t {threads} --tmpdir $(dirname {output})/tmp &> {log}"

rule copy_fasta:
    input:   "assembly/%s/assembly.fasta" % ASSEMBLER
    output:  "profile/assembly.fasta"
    shell: "cp {input} {output}"


#cut contigs by ORFs and get the bed file
rule cut_contigs:
    input:  fa="profile/assembly.fasta",
            gff="annotation/assembly.gff"
    output: contig="profile/split.fasta",
            contig_bed="profile/split.bed"
    shell:  "{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.contig_bed} -c {FRAGMENT_SIZE} > {output.contig}"

# ---- sort .bed file to reduce memory usage  ----------------------------------------------------
rule bedtools_sort:
    input:   bed="profile/split.bed"
    output:  "profile/split.sorted.bed"
    log:     "profile/split.sorted.log"
    shell:   "sort -k1,1 -k2,2n {input.bed} > {output} 2>{log} "

# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools_split_cov:
    input:   sample="profile/assembly/{sample}.sorted.bam",
             bed="profile/split.sorted.bed"
    output:  "profile/split/{sample}.cov"
    log:     "profile/split/{sample}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.sample} -mean > {output} 2>{log} "

# ---- use a awk onliner to regroup all coverages into a unique file -----------------------
rule coverage:
    input:   expand("profile/split/{sample}.cov", sample = SAMPLES)
    output:  "profile/split/coverage.tsv"
    shell : """
            echo -e "contig\t""$(ls -U {input} | cut -f1 -d "." | rev | cut -f1 -d "/" |rev | tr "\n" "\t" | sed 's/\t$//')"> {output}
            awk 'NR==FNR{{Matrix_coverage[1,FNR]=$4}}FNR==1{{f++}}{{Matrix_coverage[f+1,FNR]=$5}}END{{for(x=1;x<=FNR;x++){{for(y=1;y<ARGC+1;y++){{if(y<ARGC){{printf("%s\t",Matrix_coverage[y,x])}}if(y==ARGC){{printf("%s",Matrix_coverage[y,x]);print""}}}}}}}}' {input} >>{output}"""

# We take a look at all the SCG, take the median over the 36 of them, and multiply that by 2
rule initial_quantity_of_bins:
    input: "annotation/SCG.fna"
    output:"binning/concoct/ini_bins.cnt"
    shell: "{SCRIPTS}/get_num_bin_ini.py {input} {BINMULTIPLIER} {BINMAX} >{output}"

#  concoct 
rule concoct:
    input:   cov="profile/split/coverage.tsv",
             fasta="profile/split.fasta",
             bin_cnt="binning/concoct/ini_bins.cnt"
    output:  bins="binning/concoct/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
             data="binning/concoct/original_data_gt%d.csv" % MIN_CONTIG_SIZE
    log :   "binning/concoct/concoct.logs"
    threads: 1000
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b $(dirname {output.bins}) -c $(<{input.bin_cnt}) -l {MIN_CONTIG_SIZE} -t {threads} &>{log} 
             """

# generate orfs bed
rule bed_orfs:
    input:   gff="{path}/assembly.gff"
    output:  bed="{path}/assembly.bed"
    shell : "{SCRIPTS}/Gff_to_bed.py {input.gff} {output.bed}"


# path=binning
rule refine:
    input:  bins="{path}/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
            SCG="annotation/SCG.fna",
            data="{path}/original_data_gt%d.csv" % MIN_CONTIG_SIZE,
            orf_bed = "annotation/assembly.bed",
            split_bed = "profile/split.bed"
    output: R=temp("{path}/clustering_gt%dR.csv") % MIN_CONTIG_SIZE,
            table="{path}/clustering_gt%d_SCG_table.csv" % MIN_CONTIG_SIZE,
            bins_R="{path}/clustering_refine.csv",
            table_R="{path}/clustering_gt%d_SCG_table_R.csv" % MIN_CONTIG_SIZE,
    log:    "{path}/refine.log"
    threads: 1000
    shell:  """ 
            {SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} {input.orf_bed} {input.split_bed} {COG_FILE} -t {output.table} -T {MAG_QUAL_THRESHOLD}
            sed '1d' {input.bins}  > {output.R}
            cd binning/concoct
            concoct_refine ../../{output.R} ../../{input.data} ../../{output.table} -t {threads} &> ../../{log}
            cd -
            {SCRIPTS}/SCG_in_Bins.py {output.bins_R} {input.SCG} {input.orf_bed} {input.split_bed} {COG_FILE} -t {output.table_R} -T {MAG_QUAL_THRESHOLD}
            """

rule merge_contigs:
    input:   "{path}/clustering_refine.csv",
    output:  "{path}/clustering_concoct.csv"
    log:     "{path}/consensus.log"
    threads: THREADS
    shell:   "{SCRIPTS}/Consensus.py {input} >{output} 2>{log}"


rule create_bin_folders:
    input:   bin="binning/{binner}/clustering_{binner}.csv",
             fasta="annotation/SCG.fna",
             orf_bed = "annotation/assembly.bed",
             split_bed = "profile/split.bed"
    output:  mags="binning/{binner}/list_mags.tsv",
             table = "binning/{binner}/SCG_table_{binner}.csv"
    shell:   "{SCRIPTS}/SCG_in_Bins.py {input.bin} {input.fasta} {input.orf_bed} {input.split_bed} {COG_FILE} -all subgraphs/bin_init/ -l {output.mags} -T {MAG_QUAL_THRESHOLD} -t {output.table}"

rule compute_avg_cov:
    input:   "binning/%s/clustering_%s.csv"%(BINNER,BINNER)
    output:  "subgraphs/bin_init/bin_cov.tsv"
    shell:   "{SCRIPTS}/bin_cov.py {input} {output} {ASSEMBLY_K}"



# ---------- metabat2 part of the script -------------

rule generate_depth :
    input:  ["profile/assembly/%s.sorted.bam"%basename(sample) for sample in  SAMPLES]
    output: "profile/assembly/depth.txt"
    log : "profile/assembly/depth.log"
    shell: "jgi_summarize_bam_contig_depths --outputDepth {output} {input} &>{log}" 

rule metabat2 :
    input:  contig="profile/assembly.fasta",
            depth="profile/assembly/depth.txt"
    output: "binning/metabat2/bins/done"
    params: out="binning/metabat2/bins/Bin",
            min_contig_size=max(1500,MIN_CONTIG_SIZE) # metabat2 does not bin anything smaller than 1500
    threads : 20
    shell: """metabat2 -i {input.contig} -a {input.depth} -t {threads} -o {params.out} -m {params.min_contig_size}
            rename  's/Bin./Bin_/' {params.out}*
            touch {output}
            """
rule post_processing :
    input: "binning/metabat2/bins/done"
    output:"binning/metabat2/clustering_metabat2.csv"
    run:
        List_bins=glob.glob("binning/metabat2/bins/Bin*.fa")
        Handle=open(output[0],"w")
        Handle.write("contig_id,0\n")
        for file in List_bins :
            bin_name=file.split("Bin_")[-1].split('.fa')[0]
            for name,seq in SFP(open(file)) :
                Handle.write(",".join([name,bin_name])+"\n")
        Handle.close()
